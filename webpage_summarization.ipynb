{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy/IxQbKJZTKjlg8Pw/dZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samarth1642002/TEXT_Summarization_threads/blob/main/webpage_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbPTDULsXkKg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import threading\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from heapq import nlargest\n",
        "import time\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stopwords = list(STOP_WORDS)\n",
        "cue_phrases = [\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\",\"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_webpage_text(url):\n",
        "    r = requests.get(url)\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "    webpage_text = \"\"\n",
        "    for para in soup.find_all('p'):\n",
        "        webpage_text += para.get_text()\n",
        "    return webpage_text"
      ],
      "metadata": {
        "id": "K9X0eUgOXx8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_webpage(url):\n",
        "    try:\n",
        "        webpage_text = get_webpage_text(url)\n",
        "        docx = nlp(webpage_text)\n",
        "        word_frequencies = {}\n",
        "        for word in docx:\n",
        "            if word.text.lower() not in stopwords:\n",
        "                if word.text.lower() not in cue_phrases:\n",
        "                    if word.text.lower() not in word_frequencies.keys():\n",
        "                        word_frequencies[word.text.lower()] = 1\n",
        "                    else:\n",
        "                        word_frequencies[word.text.lower()] += 1\n",
        "        maximum_frequency = max(word_frequencies.values())\n",
        "        for word in word_frequencies.keys():\n",
        "            word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "        sentence_list = [sentence for sentence in docx.sents]\n",
        "        sentence_scores = {}\n",
        "        for sent in sentence_list:\n",
        "            for word in sent:\n",
        "                if word.text.lower() in word_frequencies.keys():\n",
        "                    if len(sent.text.split(' ')) < 25:\n",
        "                        if sent not in sentence_scores.keys():\n",
        "                            sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "                        else:\n",
        "                            sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "        summarized_sentences = nlargest(10, sentence_scores, key=sentence_scores.get)\n",
        "        final_summary = ' '.join([w.text for w in summarized_sentences])\n",
        "        print(f\"Summary for URL {url}:\\n{final_summary}\\n\", flush=True)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred for URL {url}: {e}\\n\", flush=True)\n"
      ],
      "metadata": {
        "id": "Wt1nydyKX0yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if __name__ == '__main':\n",
        "    urls = [\n",
        "        \"https://en.wikipedia.org/wiki/History_of_India\",\n",
        "        \"https://en.wikipedia.org/wiki/India\",\n",
        "        \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Big_data\",\n",
        "        \"https://en.wikipedia.org/wiki/Data_science\",\n",
        "    ]\n",
        "\n",
        "    start_time = time.time()\n",
        "    threads = []\n",
        "\n",
        "    for i, url in enumerate(urls):\n",
        "        thread = threading.Thread(target=summarize_and_print, args=(url, i))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    print(f\"Total execution time: {time.time() - start_time} seconds\")"
      ],
      "metadata": {
        "id": "tzwXFJF4X3rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}